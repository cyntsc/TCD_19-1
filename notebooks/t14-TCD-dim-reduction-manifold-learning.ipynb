{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial 3.Principal component analysis (PCA) and t-distributed Stochastic Neighbor Embedding (t-SNE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Created by Emanuel Flores-Bautista 2018.  All code contained in this notebook is licensed under the MIT licence all other content is distributed with the [Creative Commons License 4.0](https://creativecommons.org/licenses/by/4.0/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data scientists are commonly faced with the problem of finding relationships of high-dimensional data, without previous knowledge. These set of problems can be solved with methods of the so called unsupervised machine learning. To get a sense of this problem, let's watch this video from Google Researchers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T15:44:03.666703Z",
     "start_time": "2019-05-25T10:44:03.353888-05:00"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "YouTubeVideo('wvsE8jm1GzE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dimension reduction is the process by which of embedding high dimensional data into a lower dimension space where each dimension is formed by a combination of the original dimensions. Principal component analysis (PCA) is a linear transformation such that the variance between points is maximized. Let's make use of PCA on Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T15:44:08.379222Z",
     "start_time": "2019-05-25T10:44:03.670683-05:00"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as st\n",
    "import matplotlib.pyplot as plt\n",
    "import mpl_toolkits.mplot3d\n",
    "import seaborn as sns\n",
    "import sklearn.datasets\n",
    "import sklearn.decomposition\n",
    "\n",
    "import TCD19_utils as TCD_19\n",
    "\n",
    "TCD_19.set_plotting_style()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T15:44:08.410429Z",
     "start_time": "2019-05-25T10:44:08.382848-05:00"
    }
   },
   "outputs": [],
   "source": [
    "#Setting all the plots in the notebook\n",
    "%matplotlib inline\n",
    "\n",
    "#Make the figure format appear as svg\n",
    "%config InlineBackend.figure_format = 'svg' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will be using the famous Iris dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T15:44:08.423300Z",
     "start_time": "2019-05-25T10:44:08.413873-05:00"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Import the iris dataset from scikit \n",
    "\n",
    "iris= sklearn.datasets.load_iris()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the story behind this data set [here](https://en.wikipedia.org/wiki/Iris_flower_data_set), or we can use the `.DESCR` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T15:44:08.432676Z",
     "start_time": "2019-05-25T10:44:08.426153-05:00"
    }
   },
   "outputs": [],
   "source": [
    "#Print Iris dataset description \n",
    "print(iris.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, this is a data set of quantitative data about three different species of the *Iris* genus flowers made by the famous statician Ronald Fisher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T15:44:08.447639Z",
     "start_time": "2019-05-25T10:44:08.436454-05:00"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "Image(url= \"https://upload.wikimedia.org/wikipedia/commons/a/ad/Iris_persica_%28Sowerby%29.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T15:44:08.457840Z",
     "start_time": "2019-05-25T10:44:08.451213-05:00"
    }
   },
   "outputs": [],
   "source": [
    "type(iris)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's transform this `Bunch`object to a data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T15:44:08.502597Z",
     "start_time": "2019-05-25T10:44:08.460886-05:00"
    }
   },
   "outputs": [],
   "source": [
    "df_iris= pd.DataFrame(iris.data, columns= iris.feature_names)\n",
    "\n",
    "\n",
    "df_iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T15:44:08.544742Z",
     "start_time": "2019-05-25T10:44:08.507201-05:00"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_iris.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's add the species data to visualize our data set.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T15:44:08.566049Z",
     "start_time": "2019-05-25T10:44:08.548203-05:00"
    }
   },
   "outputs": [],
   "source": [
    "df_iris['species']= iris.target_names[iris.target]\n",
    "\n",
    "df_iris.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do some EDA using seaborn's `pairplot()` function to make paiwise comparisons b/w our variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T15:44:08.577332Z",
     "start_time": "2019-05-25T10:44:08.568706-05:00"
    }
   },
   "outputs": [],
   "source": [
    "palette = sns.cubehelix_palette(10, as_cmap= False)\n",
    "cmap = sns.cubehelix_palette(10, as_cmap= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T15:44:12.231504Z",
     "start_time": "2019-05-25T10:44:08.580724-05:00"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "_= sns.pairplot(df_iris, hue= 'species', palette= 'pastel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T15:44:29.022587Z",
     "start_time": "2019-05-25T10:44:12.234583-05:00"
    }
   },
   "outputs": [],
   "source": [
    "g = sns.PairGrid(df_iris, hue = 'species', palette = palette)\n",
    "\n",
    "g = g.map_lower(sns.kdeplot, cmap=cmap, n_levels=6 )\n",
    "\n",
    "g = g.map_upper(plt.scatter)\n",
    "\n",
    "g = g.map_diag(sns.violinplot)\n",
    "\n",
    "g = g.add_legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that across different variables, the versicolor and virginica species cluster together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are clearly two clusters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can do the *manual* PCA anytime we want, but for now we'll use Scikit-learn "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit uses Python object orientation: \n",
    "\n",
    "We first instatiate a `sklearn.decomposition.PCA` object and then use the `.fit()` method to get the PCA on our data. Finally we will transform the our data to a PCA object, and then to a data frame. The attributes of the PCA instance are the computed values. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T15:44:29.055021Z",
     "start_time": "2019-05-25T10:44:29.024968-05:00"
    }
   },
   "outputs": [],
   "source": [
    "#Calculate the PCs using scikit-learn\n",
    "pca= sklearn.decomposition.PCA()\n",
    "pca.fit(df_iris[iris.feature_names])\n",
    "print('Variance percent explained\\n', pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the highest variance goes to the first PCs. Each PC is a linear combination of the original variables. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we transform our data, let's select(like list slicing) only the numerical variables using the `iloc` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T15:44:29.077589Z",
     "start_time": "2019-05-25T10:44:29.058882-05:00"
    }
   },
   "outputs": [],
   "source": [
    "df_iris.iloc[:,:4].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're ready to transform our data using PCA and work with only two components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T15:44:29.089177Z",
     "start_time": "2019-05-25T10:44:29.081015-05:00"
    }
   },
   "outputs": [],
   "source": [
    "pca = sklearn.decomposition.PCA(n_components= 2)\n",
    "pca.fit=(df_iris.iloc[:,:4])\n",
    "pca_2d = pca.fit_transform(df_iris.iloc[:,:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's convert the PCA object to a data frame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T15:44:29.111413Z",
     "start_time": "2019-05-25T10:44:29.092477-05:00"
    }
   },
   "outputs": [],
   "source": [
    "df_2D= pd.DataFrame(pca_2d,\n",
    "                   columns=['PC1', 'PC2'])\n",
    "\n",
    "##Re-adding the species column to plot using labels. \n",
    "df_2D['species'] = df_iris['species']\n",
    "df_2D.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's visualize our transformed data on the first two PCs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T15:44:30.078899Z",
     "start_time": "2019-05-25T10:44:29.114727-05:00"
    }
   },
   "outputs": [],
   "source": [
    "_= sns.pairplot(df_2D, hue= 'species', palette= 'pastel')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Caveat for using PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this data set PCA worked nicely, i.e. we can see a clear difference between the different flower species. Moreover, note that the relationship of the versicolor species clustering together with the virginica mantains. PCA is a powerful method, but it may not work with other high dimensional data sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the perfect human with PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part of the tutorial was based on a [blog post](https://liorpachter.wordpress.com/tag/principal-component-analysis/) by Prof. Lior Pachter. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In brief: \n",
    "\n",
    "SNPedia is a database for human SNPs classified as good or bad. \n",
    "\n",
    "\"The perfect human\" label was generated by setting all the alleles to \"good\", he then added the perfect human to a panel of genotyped individuals from across a variety of populations and performed PCA to reveal the location and population of origin of the individual to this hypothetical perfect human...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T15:44:30.087342Z",
     "start_time": "2019-05-25T10:44:30.081429-05:00"
    }
   },
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T15:44:31.336453Z",
     "start_time": "2019-05-25T10:44:30.090387-05:00"
    }
   },
   "outputs": [],
   "source": [
    "##Load in the data, in a tidy format\n",
    "\n",
    "df_snp = pd.read_csv('../data/1_geno_table.txt', delimiter= '\\t', index_col= 'snp_id').transpose()\n",
    "\n",
    "df_snp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T15:44:31.350961Z",
     "start_time": "2019-05-25T10:44:31.339660-05:00"
    }
   },
   "outputs": [],
   "source": [
    "df_snp.shape "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that this is a high dimensional data set of 4967 variables (SNPs).Because we will do PCA on this data, we should turn the integers to floating points inside the data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T15:44:31.470026Z",
     "start_time": "2019-05-25T10:44:31.354878-05:00"
    }
   },
   "outputs": [],
   "source": [
    "df_snp= df_snp.astype(np.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also have the info for each individual, in the *.panel file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T15:44:31.499369Z",
     "start_time": "2019-05-25T10:44:31.472949-05:00"
    }
   },
   "outputs": [],
   "source": [
    "fname = '../data/2_integrated_call_samples_v3.20130502.ALL.panel'\n",
    "\n",
    "df_info = pd.read_csv(fname, delimiter= '\\t', index_col=0).dropna(axis= 1, how = 'all')\n",
    "\n",
    "df_info.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenience we will merge the two dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T15:44:31.513284Z",
     "start_time": "2019-05-25T10:44:31.502139-05:00"
    }
   },
   "outputs": [],
   "source": [
    "##Add to the SNP df, the info about gender and population\n",
    "\n",
    "aux_cols = ['pop', 'super_pop', 'gender']\n",
    "\n",
    "df_snp[aux_cols] = df_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add a row corresponding to the perfect human."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T15:44:31.538450Z",
     "start_time": "2019-05-25T10:44:31.517274-05:00"
    }
   },
   "outputs": [],
   "source": [
    "#Assign the super_pop and pop column to 'perfect human being'\n",
    "df_snp.loc['perfect', ['pop', 'super_pop']] = ['Perfect Human']*2\n",
    "\n",
    "##Take a look at aux cols\n",
    "\n",
    "df_snp[aux_cols].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalize the data using Standard Scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA is not scale invariant i.e. If your dataset contains thigs measured in nanometers and things measured in meters, or things measuring completely unrelated things, the units would affect the PCA analysis. \n",
    "\n",
    "The simplest way to avoid this is to form a \"common set of units\" by stardadizing or normalizing your data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T15:44:44.623825Z",
     "start_time": "2019-05-25T10:44:31.541658-05:00"
    }
   },
   "outputs": [],
   "source": [
    "df_snp.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a standard scaler, we set each column to a uniform gaussian distribution with $\\mu$ = 0 and $\\sigma$  = 1. Before "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T15:44:44.933112Z",
     "start_time": "2019-05-25T10:44:44.627996-05:00"
    }
   },
   "outputs": [],
   "source": [
    "# Normalize data, use the.drop() method to remove the categorical variables. \n",
    "df_snp_std = sklearn.preprocessing.StandardScaler().fit_transform(\n",
    "    df_snp.drop(aux_cols, axis = 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-05-25T15:44:03.772Z"
    }
   },
   "outputs": [],
   "source": [
    "#Perform the PCA and transform the data\n",
    "\n",
    "n_components = 5\n",
    "\n",
    "snp_pca = sklearn.decomposition.PCA(n_components= n_components)## initialize the PCA obj.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-05-25T15:44:03.781Z"
    }
   },
   "outputs": [],
   "source": [
    "#Fit/transform\n",
    "\n",
    "snp_pca.fit(df_snp_std)\n",
    "\n",
    "df_snp_pca = snp_pca.transform(df_snp_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-05-25T15:44:03.791Z"
    }
   },
   "outputs": [],
   "source": [
    "#Convert back to a nice, tidy dataframe\n",
    "\n",
    "df_snp_pca = pd.DataFrame(df_snp_pca, \n",
    "                         columns= ['PC' +  str(x) for x in range(1, n_components+1)],\n",
    "                         index= df_snp.index)\n",
    "\n",
    "#Add again the metadata (gender, pop, and super_pop columns)\n",
    "\n",
    "df_snp_pca[aux_cols] = df_snp[aux_cols]\n",
    "\n",
    "df_snp_pca.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the first two components and find out who the perfect human being $is$ ! First let's import some amazing interactive visualization modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-05-25T15:44:03.798Z"
    }
   },
   "outputs": [],
   "source": [
    "import holoviews as hv\n",
    "import bebi103\n",
    "import bokeh.io\n",
    "import bokeh.plotting\n",
    "# Set up Bokeh for inline viewing\n",
    "bokeh.io.output_notebook()\n",
    "hv.extension('bokeh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-05-25T15:44:03.807Z"
    }
   },
   "outputs": [],
   "source": [
    "%%opts Scatter [show_grid=True, width=500, height=420, tools=['hover']] (size=5)\n",
    "\n",
    "\n",
    "scatter = hv.Scatter(df_snp_pca,\n",
    "                     kdims=['PC1'], \n",
    "                     vdims=['PC2', 'pop', 'super_pop', 'gender'])\n",
    "\n",
    "# Make groupby object\n",
    "gb = scatter.groupby('super_pop')\n",
    "\n",
    "# Make the overlay method\n",
    "overlay = gb.overlay()\n",
    "\n",
    "# Display\n",
    "overlay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the super pops cluster together. \n",
    "\n",
    "By zooming in to the perfect human, the nearest neighbors are two Puerto Rican women and one man! After Lior published these results, the media immediately blew it out of proportion (and context). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-05-25T15:44:03.818Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML \n",
    "\n",
    "HTML('<iframe src=https://www.huffingtonpost.com/julio-pabon/the-closet-perfect-human-_b_6304366.html width=700 height=350></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we plot other PCs we can see that this transformation of data is not so reliable among higher dimensional datasets, and that the perfect human is really an outlier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-05-25T15:44:03.828Z"
    }
   },
   "outputs": [],
   "source": [
    "%%opts Scatter [show_grid=True, width=500, height=450, tools=['hover']] (size=5)\n",
    "\n",
    "\n",
    "scatter = hv.Scatter(df_snp_pca,\n",
    "                     kdims=['PC1'], \n",
    "                     vdims=['PC3', 'pop', 'super_pop', 'gender'])\n",
    "\n",
    "# Make groupby object\n",
    "gb = scatter.groupby('super_pop')\n",
    "\n",
    "# Make the overlay method\n",
    "overlay = gb.overlay()\n",
    "\n",
    "# Display\n",
    "overlay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize this *outlier-ness* by plotting the first three PCs together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-05-25T15:44:03.836Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(1, figsize=(8, 6))\n",
    "ax = mpl_toolkits.mplot3d.Axes3D(fig)\n",
    "for key, group in df_snp_pca.groupby(['super_pop']):\n",
    "    if key == 'Perfect Human':\n",
    "        ax.plot(group.PC1, group.PC2, group.PC3, 'k*', markersize=15,\n",
    "                label=key)\n",
    "    else:\n",
    "        ax.plot(group.PC1, group.PC2, group.PC3, 'o', alpha=0.05, label=key)\n",
    "\n",
    "ax.set_title(\"First three PC directions\", fontsize = 21)\n",
    "ax.set_xlabel(\"PC 1\")\n",
    "ax.set_ylabel(\"PC 2\")\n",
    "ax.set_zlabel(\"PC 3\")\n",
    "ax.legend(loc='best', fontsize=12);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Lior's words: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Here the “perfect human” is revealed to be decidedly non-human. This is not surprising, and it reflects the fact that the alleles of the “perfect human” place it as significant outlier to the human population. In fact, this is even more evident in the case of the “worst human”, namely the individual that has the “bad” alleles at every SNPs. A projection of that individual onto any combination of principal components shows them to be far removed from any actual human. The best visualization appears in the projection onto the 2nd and 3rd principal components, where they appear as a clear outlier (point labeled DYS), and diametrically opposite to Africans:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying t-SNE to SNP data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t-SNE is a non-linear dimension reduction method, that is more powerful than PCA for some applications. It is also a good technique to visualize high dimensional data. Let's apply t-SNE on the SNP data and visualize it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a sense of the way t-SNE works, watch [this amazing talk](https://youtu.be/aStvaXMhGGs) by Gal Yona at Tel Aviv PyData 2017."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-05-25T15:44:03.844Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE as tsne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-05-25T15:44:03.849Z"
    }
   },
   "outputs": [],
   "source": [
    "#Make the object\n",
    "\n",
    "t_sne = tsne(n_components= 3, init= 'pca', random_state= 42, learning_rate = 40)\n",
    "\n",
    "#Transform the SNP data to t-SNE form\n",
    "t_snp= t_sne.fit_transform(df_snp_std)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-05-25T15:44:03.857Z"
    }
   },
   "outputs": [],
   "source": [
    "# Convert to DataFrame\n",
    "df_tsne_SNP = pd.DataFrame(t_snp, \n",
    "                         columns= ['t-SNE' +  str(x) for x in range(1, 4)],\n",
    "                         index= df_snp.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-05-25T15:44:03.861Z"
    }
   },
   "outputs": [],
   "source": [
    "df_tsne_SNP['super_pop'] = df_snp_pca['super_pop']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-05-25T15:44:03.866Z"
    }
   },
   "outputs": [],
   "source": [
    "df_tsne_SNP['pop'] = df_snp_pca['pop']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-05-25T15:44:03.872Z"
    }
   },
   "outputs": [],
   "source": [
    "df_tsne_SNP['gender'] = df_snp_pca['gender']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-05-25T15:44:03.876Z"
    }
   },
   "outputs": [],
   "source": [
    "%%opts Scatter [show_grid=True, width=500, height=450, tools=['hover']] (size=5)\n",
    "\n",
    "\n",
    "scatter = hv.Scatter(df_tsne_SNP,\n",
    "                     kdims=['t-SNE1'], \n",
    "                     vdims=['t-SNE2', 'pop', 'super_pop', 'gender'])\n",
    "\n",
    "# Make groupby object\n",
    "gb = scatter.groupby('super_pop')\n",
    "\n",
    "# Make the overlay method\n",
    "overlay = gb.overlay()\n",
    "\n",
    "# Display\n",
    "overlay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We can see that in this case, t-SNE doesn't work as good as PCA. However other times is exactly the opposite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-05-25T15:44:03.882Z"
    }
   },
   "outputs": [],
   "source": [
    "?sklearn.manifold.TSNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can try to tweak the learning rate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-05-25T15:44:03.887Z"
    }
   },
   "outputs": [],
   "source": [
    "t_sne = tsne(n_components= 3, init= 'pca', random_state= 42, learning_rate = 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-05-25T15:44:03.891Z"
    }
   },
   "outputs": [],
   "source": [
    "t_snp= t_sne.fit_transform(df_snp_std)\n",
    "\n",
    "df_tsne_SNP = pd.DataFrame(t_snp, \n",
    "                         columns= ['t-SNE' +  str(x) for x in range(1, 4)],\n",
    "                         index= df_snp.index)\n",
    "\n",
    "df_tsne_SNP['super_pop'] = df_snp_pca['super_pop']\n",
    "df_tsne_SNP['pop'] = df_snp_pca['pop']\n",
    "df_tsne_SNP['gender'] = df_snp_pca['gender']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-05-25T15:44:03.895Z"
    }
   },
   "outputs": [],
   "source": [
    "%%opts Scatter [show_grid=True, width=500, height=450, tools=['hover']] (size=5)\n",
    "\n",
    "\n",
    "scatter = hv.Scatter(df_tsne_SNP,\n",
    "                     kdims=['t-SNE1'], \n",
    "                     vdims=['t-SNE2', 'pop', 'super_pop', 'gender'])\n",
    "\n",
    "# Make groupby object\n",
    "gb = scatter.groupby('super_pop')\n",
    "\n",
    "# Make the overlay method\n",
    "overlay = gb.overlay()\n",
    "\n",
    "# Display\n",
    "overlay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the data is a bit more sparse, but it's still a bit too ball-like. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dimensionality reduction works for some things, for others... not so well. In general, I wouldn't recommend to make any strong conclusions from dimensionality reduction visualizations, unless you have a lot of certainty about the data, and can afford to do so. However, I would strongly suggest to do dimensionality reduction to *massage* your data prior to making some other machine learning method, such as clustering, classification or regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
