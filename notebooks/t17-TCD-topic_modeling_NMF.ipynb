{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural language processing and non-negative matrix factorization (NMF) for topic modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T08:22:23.290330Z",
     "start_time": "2019-05-25T03:22:17.813463-05:00"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "import TCD19_utils as TCD\n",
    "TCD.set_plotting_style_2()\n",
    "\n",
    "# Magic command to enable plotting inside notebook\n",
    "%matplotlib inline\n",
    "# Magic command to enable svg format in plots\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "#Set random seed\n",
    "seed = np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural language processing (NLP) vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text is by itself an unstructured data type. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* corpus: set of documents\n",
    "* stop words: \n",
    "* tokenization: Tokenization breaks unstructured data, in this case text, into chunks of information which can be counted as discrete elements. These counts of token occurrences in a document can be used directly as a vector representing that document. This immediately turns an unstructured string (text document) into a structured, numerical data structure suitable for machine learning. \n",
    "* n-gram : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T08:22:25.452928Z",
     "start_time": "2019-05-25T03:22:23.313929-05:00"
    }
   },
   "outputs": [],
   "source": [
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1,\n",
    "                             remove=('headers', 'footers', 'quotes'))\n",
    "documents = dataset.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T08:22:27.726722Z",
     "start_time": "2019-05-25T03:22:27.718469-05:00"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T08:25:00.664611Z",
     "start_time": "2019-05-25T03:25:00.656487-05:00"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'                                      ^^^^^^\\nNo argument at all with Murphy.  He scared the hell out of me when he came in\\nlast year.  On the other hand, the club though enough of Boever to put him into\\nan awful lot of games (he may have led the league in appearances - he did at\\nleast at some point).  He seemed to be a very viable setup guy - but I guess\\nthat\\'s not considered that crucial by the club.  I can just remember two years\\nago so well, though...\\n...\\n\\nI\\'m not that concerned.  Those guys have been relatively consistent over the\\nyears and they have no good reasons to decline (no injuries, not old, ...).\\nI expect them to come through just fine.  It\\'s those guys that have not\\nbeen consistently good that are the worrisome part, even if they are coming\\nthrough right now.\\n\\nThis sounds like their old road unis.  Pretty dull.  Buttons or pullovers?\\nI\\'ll check through my uniform book to see if they\\'ve always had some orange.\\n\\n\\nWell, we\\'ll see.  I\\'ve got a Astros pullover shirt with the \"Astros stripes\"\\nacross the shoulders and I have trouble making myself wear it in public.  i\\ncan see why they might want that to change.  Gee, if they eliminate the\\norange, will they reupholster the seats in the Astros stripes section (what\\nused to be the gold and yellow levels - I don\\'t know those numbers they use\\nnow).\\n\\nI saw a pinstripe version of an Astros cap and I actually thought it looked \\ngood!'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that each element of the list is a document containing a news article. We will now proceed to transform our document list to a term frequency-inverse document frequency (TF-IDF) matrix. This matrix has a  (`docs`, `words`) shape, where the words will be our features. The values inside the matrix will be TF-IDF that is defined by the following formula. \n",
    "\n",
    "\\begin{align}\n",
    "\\text{TF-IDF} = \\text{TF} \\times \\log{ \\frac{N}{DF}}\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "Where: \n",
    "$\\text{TF} = \\text{word} \\text{counts in} \\, document_{i}$,  $\\text{DF (document frequency) =  counts of the documents that contain} \\, word_{i} $ , $\\text{N = total number of documents}$. \n",
    "\n",
    "Notice that if a word is ubiquitous in all documents (e.g. stop words), the term in the right turns to zero. In this sense TF-IDF makes a robust assesment of the word frequency in a document, eliminating the bias of highly repeated words. This makes sense as ubiquitous words throughout different documents might not contain any information about the documents themselves. \n",
    "\n",
    "The sci-kit learn package has a great implementation using the `tfidf_vectorizer`. Some important arguments of this function are `max_df` threshold for the proportion of documents that have highly repeated words, (`min_df`is the low limit cutoff, if `int` it is in counts), `stop_words` let's you pick a language to eliminate stop words from, and `max_features` let's you consider the top words term frequency across the corpus.\n",
    "\n",
    "Let's compute the TF-IDF vectorizer on our documents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-30T03:15:06.891439Z",
     "start_time": "2019-04-29T22:15:03.737030-05:00"
    }
   },
   "outputs": [],
   "source": [
    "no_features = 1000\n",
    "\n",
    "# NMF is able to use tf-idf\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2,\n",
    "                                   max_features=no_features,\n",
    "                                   stop_words='english')\n",
    "\n",
    "tfidf = tfidf_vectorizer.fit_transform(documents)\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-30T03:15:06.907705Z",
     "start_time": "2019-04-29T22:15:06.894656-05:00"
    }
   },
   "outputs": [],
   "source": [
    "type(tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our TF-IDF matrix is a scipy sparse matrix and that we cannot readily visualize it's components. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-30T03:15:06.918482Z",
     "start_time": "2019-04-29T22:15:06.911127-05:00"
    }
   },
   "outputs": [],
   "source": [
    "tfidf_feature_names[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our features are words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-30T03:15:07.132616Z",
     "start_time": "2019-04-29T22:15:06.921630-05:00"
    }
   },
   "outputs": [],
   "source": [
    "df_tfidf = pd.DataFrame(tfidf.toarray(), columns=tfidf_feature_names)\n",
    "\n",
    "df_tfidf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-30T03:15:07.144688Z",
     "start_time": "2019-04-29T22:15:07.136054-05:00"
    }
   },
   "outputs": [],
   "source": [
    "df_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to extract the topics from our documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-negative matrix factorization (NMF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NMF can be thought of as a clustering technique, as  it finds a decomposition of samples $\\matrix{X}$ into two matrices $\\matrix{W}$ and $\\matrix{H}$ of non-negative elements, by minimizing the distance d between $\\matrix{X}$ and the matrix product $\\matrix{W}$$\\matrix{H}$ . More specifically each matrix represents the following:\n",
    "\n",
    "$\\matrix{W}$ (clusters) = the topics (clusters) discovered from the documents.\n",
    "$\\matrix{H}$ (coefficient matrix) = the membership weights for the topics in each document.\n",
    "$\\matrix{X}$ (Document-word matrix) — input that contains which words appear in which documents.\n",
    "\n",
    "\\begin{align}\n",
    "\\matrix{X} = \\matrix{W}\\matrix{H}\n",
    "\\end{align}\n",
    "\n",
    "We won't go to any mathematical detail but just know that the current implementation in scikit learn uses the minimization of the Frobenius norm, the matrix analog of the euclidean distance, and that you can use different divergence measurements like the Kullback-leibler divergence by modifying the `beta_loss` parameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-30T03:15:07.155641Z",
     "start_time": "2019-04-29T22:15:07.150857-05:00"
    }
   },
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-30T03:15:07.167506Z",
     "start_time": "2019-04-29T22:15:07.159021-05:00"
    }
   },
   "outputs": [],
   "source": [
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-30T03:15:09.278149Z",
     "start_time": "2019-04-29T22:15:07.170306-05:00"
    }
   },
   "outputs": [],
   "source": [
    "n_topics = 10\n",
    "\n",
    "# Run NMF on the TF-IDF matrix\n",
    "nmf = NMF(n_components=n_topics).fit(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-30T03:15:09.332383Z",
     "start_time": "2019-04-29T22:15:09.280779-05:00"
    }
   },
   "outputs": [],
   "source": [
    "#Transform\n",
    "nmf_W = nmf.transform(tfidf)\n",
    "nmf_H = nmf.components_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-30T03:15:09.342989Z",
     "start_time": "2019-04-29T22:15:09.334955-05:00"
    }
   },
   "outputs": [],
   "source": [
    "print('Topic (W) matrix has a', nmf_W.shape , 'shape')\n",
    "print('Coeffficient (H) matrix has a ', nmf_H.shape, 'shape')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-29T01:40:05.431709Z",
     "start_time": "2019-04-28T20:40:05.424871-05:00"
    }
   },
   "source": [
    "We can see that the $\\matrix{W}$ matrix has a (`n_documents`, `n_topics`) shape.\n",
    "\n",
    "We can readily see that the NMF $\\matrix{H}$ matrix has a shape of (`n_topics`, `n_features`) .\n",
    "\n",
    "Therefore if we want to get the topic associations from each document we must get the biggest argument from the topic matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-30T03:15:09.370650Z",
     "start_time": "2019-04-29T22:15:09.346521-05:00"
    }
   },
   "outputs": [],
   "source": [
    "nmf_topics = []\n",
    "for i in range(nmf_W.shape[0]):\n",
    "    nmf_topics.append(nmf_W[i].argmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-30T03:15:09.381806Z",
     "start_time": "2019-04-29T22:15:09.373543-05:00"
    }
   },
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, no_top_words):\n",
    "    \n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        \n",
    "        #print topic index\n",
    "        print (\"Topic %d:\" % (topic_idx))\n",
    "        \n",
    "        #print topic \n",
    "        print (\" \".join([feature_names[i] for i in topic.argsort()[: - no_top_words -1 :-1]]))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-30T03:15:09.397341Z",
     "start_time": "2019-04-29T22:15:09.385236-05:00"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "no_top_words = 15\n",
    "display_topics(nmf, tfidf_feature_names, no_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voilà. We have our topics and the most important words associated with them. From this we can readily make a list that will serve for visualization purposes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-30T03:19:34.954138Z",
     "start_time": "2019-04-29T22:19:34.937800-05:00"
    }
   },
   "outputs": [],
   "source": [
    "classes = ['random','video','catholic church','gamers', 'bike / car selling',\n",
    "           'email','windows','computer science','cybersecurity',\n",
    "           'hardware']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's see if by using a PCA on the topic matrix, we can visualize the documents in its document space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-30T03:15:09.416742Z",
     "start_time": "2019-04-29T22:15:09.410295-05:00"
    }
   },
   "outputs": [],
   "source": [
    "palette = TCD.palette(cmap = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-30T03:15:09.441543Z",
     "start_time": "2019-04-29T22:15:09.420110-05:00"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(random_state = 42)\n",
    "\n",
    "doc_pca = pca.fit_transform(nmf_W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-30T15:11:14.314690Z",
     "start_time": "2019-04-30T10:11:02.300815-05:00"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8,6))\n",
    "\n",
    "plt.scatter(doc_pca[:,0], doc_pca[:, 1], alpha = 0.8,\n",
    "            c = nmf_topics, cmap = palette.reversed())\n",
    "\n",
    "plt.xlabel('PCA 1')\n",
    "plt.ylabel('PCA 2')\n",
    "cbar = plt.colorbar(boundaries=np.arange(11)-0.5)\n",
    "cbar.set_ticks(np.arange(10))\n",
    "cbar.set_ticklabels(classes)\n",
    "\n",
    "#plt.savefig('news_pca.png', dpi = 500, bbox_inches = 'tight')\n",
    "plt.tight_layout();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-30T03:15:56.062575Z",
     "start_time": "2019-04-29T22:15:53.898132-05:00"
    }
   },
   "outputs": [],
   "source": [
    "from umap import UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-30T15:10:34.480821Z",
     "start_time": "2019-04-30T10:10:14.690777-05:00"
    }
   },
   "outputs": [],
   "source": [
    "reducer = UMAP(random_state = 42)\n",
    "\n",
    "doc_umap = reducer.fit_transform(nmf_W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-30T15:10:47.599124Z",
     "start_time": "2019-04-30T10:10:35.028385-05:00"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8,6))\n",
    "\n",
    "plt.scatter(doc_umap[:,0], doc_umap[:, 1], alpha = 0.8, \n",
    "            c = nmf_topics, cmap = palette.reversed())\n",
    "\n",
    "plt.xlabel('UMAP 1')\n",
    "plt.ylabel('UMAP 2')\n",
    "cbar = plt.colorbar(boundaries=np.arange(11)-0.5)\n",
    "cbar.set_ticks(np.arange(10))\n",
    "cbar.set_ticklabels(classes)\n",
    "\n",
    "#plt.savefig('news_UMAP.png', dpi = 500, bbox_inches = 'tight')\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reducer = UMAP(random_state = 42, y = nmf_topics)\n",
    "\n",
    "doc_umap = reducer.fit_transform(nmf_W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8,6))\n",
    "\n",
    "plt.scatter(doc_umap[:,0], doc_umap[:, 1], alpha = 0.8, \n",
    "            c = nmf_topics, cmap = palette.reversed())\n",
    "\n",
    "plt.xlabel('UMAP 1')\n",
    "plt.ylabel('UMAP 2')\n",
    "cbar = plt.colorbar(boundaries=np.arange(11)-0.5)\n",
    "cbar.set_ticks(np.arange(10))\n",
    "cbar.set_ticklabels(classes)\n",
    "\n",
    "#plt.savefig('news_UMAP_learn.png', dpi = 500, bbox_inches = 'tight')\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have seen an end to end clustering and visualization pipeline using NMF, PCA, and UMAP. We could also use another method for topic modelling in text called Latent Dirichlet Allocation (LDA). The implementation is very similar, but if you want to see how to do so follow this [great post](https://medium.com/mlreview/topic-modeling-with-scikit-learn-e80d33668730). \n",
    "\n",
    "Other than clustering, NMF can be applied for collaborative filtering and image analysis. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
