{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial 20. Sentiment analysis\n",
    "\n",
    "Created by Emanuel Flores-Bautista 2019  All content contained in this notebook is licensed under a [Creative Commons License 4.0 BY NC](https://creativecommons.org/licenses/by-nc/4.0/). The code is licensed under a [MIT license](https://opensource.org/licenses/MIT).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T09:23:21.129655Z",
     "start_time": "2019-05-25T04:23:15.843065-05:00"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as st\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import classification_report,accuracy_score,confusion_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from keras.datasets import imdb\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T09:23:21.145061Z",
     "start_time": "2019-05-25T04:23:21.136923-05:00"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will train a classifier movie for reviews in the IMDB data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T09:23:28.999880Z",
     "start_time": "2019-05-25T04:23:21.149363-05:00"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset with 25000 training samples,25000 test samples\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = 5000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words = vocabulary_size)\n",
    "print('Loaded dataset with {} training samples,{} test samples'.format(len(X_train), len(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T09:23:29.017325Z",
     "start_time": "2019-05-25T04:23:29.003892-05:00"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "218"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T09:23:29.033595Z",
     "start_time": "2019-05-25T04:23:29.023690-05:00"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---review---\n",
      "[1, 2, 365, 1234, 5, 1156, 354, 11, 14, 2, 2, 7, 1016, 2, 2, 356, 44, 4, 1349, 500, 746, 5, 200, 4, 4132, 11, 2, 2, 1117, 1831, 2, 5, 4831, 26, 6, 2, 4183, 17, 369, 37, 215, 1345, 143, 2, 5, 1838, 8, 1974, 15, 36, 119, 257, 85, 52, 486, 9, 6, 2, 2, 63, 271, 6, 196, 96, 949, 4121, 4, 2, 7, 4, 2212, 2436, 819, 63, 47, 77, 2, 180, 6, 227, 11, 94, 2494, 2, 13, 423, 4, 168, 7, 4, 22, 5, 89, 665, 71, 270, 56, 5, 13, 197, 12, 161, 2, 99, 76, 23, 2, 7, 419, 665, 40, 91, 85, 108, 7, 4, 2084, 5, 4773, 81, 55, 52, 1901]\n",
      "---label---\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print('---review---')\n",
    "print(X_train[6])\n",
    "print('---label---')\n",
    "print(y_train[6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the review is stored as a sequence of integers. From the [Keras documentation](https://keras.io/datasets/) we can see that these are words IDs that have been pre-assigned to individual words, and the label is an integer (0 for negative, 1 for positive). We can go ahead and access the words from each review with the `get_word_index()` method from the `imdb` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T09:23:29.156906Z",
     "start_time": "2019-05-25T04:23:29.037042-05:00"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---review with words---\n",
      "['the', 'and', 'full', 'involving', 'to', 'impressive', 'boring', 'this', 'as', 'and', 'and', 'br', 'villain', 'and', 'and', 'need', 'has', 'of', 'costumes', 'b', 'message', 'to', 'may', 'of', 'props', 'this', 'and', 'and', 'concept', 'issue', 'and', 'to', \"god's\", 'he', 'is', 'and', 'unfolds', 'movie', 'women', 'like', \"isn't\", 'surely', \"i'm\", 'and', 'to', 'toward', 'in', \"here's\", 'for', 'from', 'did', 'having', 'because', 'very', 'quality', 'it', 'is', 'and', 'and', 'really', 'book', 'is', 'both', 'too', 'worked', 'carl', 'of', 'and', 'br', 'of', 'reviewer', 'closer', 'figure', 'really', 'there', 'will', 'and', 'things', 'is', 'far', 'this', 'make', 'mistakes', 'and', 'was', \"couldn't\", 'of', 'few', 'br', 'of', 'you', 'to', \"don't\", 'female', 'than', 'place', 'she', 'to', 'was', 'between', 'that', 'nothing', 'and', 'movies', 'get', 'are', 'and', 'br', 'yes', 'female', 'just', 'its', 'because', 'many', 'br', 'of', 'overly', 'to', 'descent', 'people', 'time', 'very', 'bland']\n",
      "---label---\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "word2id = imdb.get_word_index()\n",
    "id2word = {i: word for word, i in word2id.items()}\n",
    "print('---review with words---')\n",
    "print([id2word.get(i, ' ') for i in X_train[6]])\n",
    "print('---label---')\n",
    "print(y_train[6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we cannot feed the index matrix directly to the classifier, we need to perform some data wrangling and feature extraction abilities. We're going to write a couple of functions, in order to \n",
    "\n",
    "1. Get a list of reviews, consisting of full length strings. \n",
    "2. Perform TF-IDF feature extraction on the reviews documents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T09:23:29.176553Z",
     "start_time": "2019-05-25T04:23:29.160748-05:00"
    }
   },
   "outputs": [],
   "source": [
    "def get_joined_rvw(X):\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    Given an X_train or X_test dataset from the IMDB reviews\n",
    "    of Keras, return a list of the reviews in string format. \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    #Get word to index dictionary\n",
    "    word2id = imdb.get_word_index()\n",
    "    #Get index to word mapping dictionary\n",
    "    id2word = {i: word for word, i in word2id.items()}\n",
    "    \n",
    "    #Initialize reviews list\n",
    "    doc_list = []\n",
    "    \n",
    "    for review in X:\n",
    "        #Extract review\n",
    "        initial_rvw = [id2word.get(i) for i in review]\n",
    "        \n",
    "        #Join strings followed by spaces\n",
    "        joined_rvw = \" \".join(initial_rvw)\n",
    "        \n",
    "        #Append review to the doc_list\n",
    "        doc_list.append(joined_rvw)\n",
    "        \n",
    "    return doc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T09:23:29.201391Z",
     "start_time": "2019-05-25T04:23:29.179802-05:00"
    }
   },
   "outputs": [],
   "source": [
    "def get_data_from_keras_imdb():\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    Extract TF-IDF matrices for the Keras IMDB dataset. \n",
    "    \n",
    "    \"\"\"\n",
    "    vocabulary_size = 1000\n",
    "    (X_train, y_train), (X_test, y_test) = imdb.load_data(num_words = vocabulary_size)\n",
    "    \n",
    "    #X = np.vstack([X_train[:, None], X_test[:, None]])\n",
    "    \n",
    "    X_train_docs = get_joined_rvw(X_train)\n",
    "    X_test_docs = get_joined_rvw(X_test)\n",
    "    \n",
    "    \n",
    "    tf_idf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2,\n",
    "                                   max_features=vocabulary_size,\n",
    "                                   stop_words='english')\n",
    "    \n",
    "    tf_idf_train = tf_idf_vectorizer.fit_transform(X_train_docs)\n",
    "    \n",
    "    tf_idf_test = tf_idf_vectorizer.fit_transform(X_test_docs)\n",
    "    \n",
    "    #tf_idf_feature_names = tf_idf_vectorizer.get_feature_names() \n",
    "    \n",
    "    #tf_idf = np.vstack([tf_idf_train.toarray(), tf_idf_test.toarray()])\n",
    "    \n",
    "    #X_new = pd.DataFrame(tf_idf, columns=tf_idf_feature_names)\n",
    "    \n",
    "    X_train_new = tf_idf_train.toarray()\n",
    "    \n",
    "    X_test_new = tf_idf_test.toarray()\n",
    "\n",
    "    \n",
    "    return X_train_new, y_train, X_test_new, y_test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T09:23:51.566936Z",
     "start_time": "2019-05-25T04:23:29.204508-05:00"
    }
   },
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test  = get_data_from_keras_imdb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T09:23:51.585902Z",
     "start_time": "2019-05-25T04:23:51.575550-05:00"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataset shape (25000, 745)\n",
      "test dataset shape (25000, 745)\n"
     ]
    }
   ],
   "source": [
    "print('train dataset shape', X_train.shape)\n",
    "print('test dataset shape', X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can readily see that we are ready to train our classification algorithm with the TF-IDF matrices. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T09:24:02.217253Z",
     "start_time": "2019-05-25T04:23:51.594177-05:00"
    }
   },
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_estimators=200, max_depth=3, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T09:24:02.247522Z",
     "start_time": "2019-05-25T04:24:02.220743-05:00"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.84      0.67      0.75     12500\n",
      "          1       0.73      0.88      0.80     12500\n",
      "\n",
      "avg / total       0.79      0.78      0.77     25000\n",
      "\n",
      "Accuracy score :  0.77504\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))\n",
    "print('Accuracy score : ', accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-05-25T09:23:15.965Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.84      0.81      0.83     12500\n",
      "          1       0.82      0.84      0.83     12500\n",
      "\n",
      "avg / total       0.83      0.83      0.83     25000\n",
      "\n",
      "Accuracy score :  0.82752\n"
     ]
    }
   ],
   "source": [
    "model = MLPClassifier()\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "print('Accuracy score : ', accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-05-25T09:23:15.972Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "cross_val_score(model, X_train, y_train, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-05-25T09:23:15.977Z"
    }
   },
   "outputs": [],
   "source": [
    "import manu_utils as TCD\n",
    "palette = TCD.palette(cmap = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-05-25T09:23:15.993Z"
    }
   },
   "outputs": [],
   "source": [
    "C = confusion_matrix(y_test, y_pred)\n",
    "c_normed = C / C.astype(np.float).sum(axis=1) [:, np.newaxis]\n",
    "\n",
    "sns.heatmap(c_normed, cmap = palette, xticklabels=['negative', 'positive'], \n",
    "           yticklabels=['negative', 'positive'], annot= True, vmin = 0, vmax = 1, \n",
    "           cbar_kws = {'label': 'recall'})\n",
    "\n",
    "#\n",
    "\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
