{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial 20. Sentiment analysis\n",
    "\n",
    "Created by Emanuel Flores-Bautista 2019  All content contained in this notebook is licensed under a [Creative Commons License 4.0 BY NC](https://creativecommons.org/licenses/by-nc/4.0/). The code is licensed under a [MIT license](https://opensource.org/licenses/MIT).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T09:23:21.129655Z",
     "start_time": "2019-05-25T04:23:15.843065-05:00"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as st\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import classification_report,accuracy_score,confusion_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from keras.datasets import imdb\n",
    "import TCD19_utils as TCD\n",
    "\n",
    "TCD.set_plotting_style_2()\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T09:23:21.145061Z",
     "start_time": "2019-05-25T04:23:21.136923-05:00"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will train a classifier movie for reviews in the IMDB data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T09:23:28.999880Z",
     "start_time": "2019-05-25T04:23:21.149363-05:00"
    }
   },
   "outputs": [],
   "source": [
    "vocabulary_size = 5000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words = vocabulary_size)\n",
    "print('Loaded dataset with {} training samples,{} test samples'.format(len(X_train), len(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T09:23:29.017325Z",
     "start_time": "2019-05-25T04:23:29.003892-05:00"
    }
   },
   "outputs": [],
   "source": [
    "len(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T09:23:29.033595Z",
     "start_time": "2019-05-25T04:23:29.023690-05:00"
    }
   },
   "outputs": [],
   "source": [
    "print('---review---')\n",
    "print(X_train[6])\n",
    "print('---label---')\n",
    "print(y_train[6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the review is stored as a sequence of integers. From the [Keras documentation](https://keras.io/datasets/) we can see that these are words IDs that have been pre-assigned to individual words, and the label is an integer (0 for negative, 1 for positive). We can go ahead and access the words from each review with the `get_word_index()` method from the `imdb` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T09:23:29.156906Z",
     "start_time": "2019-05-25T04:23:29.037042-05:00"
    }
   },
   "outputs": [],
   "source": [
    "word2id = imdb.get_word_index()\n",
    "id2word = {i: word for word, i in word2id.items()}\n",
    "print('---review with words---')\n",
    "print([id2word.get(i, ' ') for i in X_train[6]])\n",
    "print('---label---')\n",
    "print(y_train[6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we cannot feed the index matrix directly to the classifier, we need to perform some data wrangling and feature extraction abilities. We're going to write a couple of functions, in order to \n",
    "\n",
    "1. Get a list of reviews, consisting of full length strings. \n",
    "2. Perform TF-IDF feature extraction on the reviews documents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T09:23:29.176553Z",
     "start_time": "2019-05-25T04:23:29.160748-05:00"
    }
   },
   "outputs": [],
   "source": [
    "def get_joined_rvw(X):\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    Given an X_train or X_test dataset from the IMDB reviews\n",
    "    of Keras, return a list of the reviews in string format. \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    #Get word to index dictionary\n",
    "    word2id = imdb.get_word_index()\n",
    "    #Get index to word mapping dictionary\n",
    "    id2word = {i: word for word, i in word2id.items()}\n",
    "    \n",
    "    #Initialize reviews list\n",
    "    doc_list = []\n",
    "    \n",
    "    for review in X:\n",
    "        #Extract review\n",
    "        initial_rvw = [id2word.get(i) for i in review]\n",
    "        \n",
    "        #Join strings followed by spaces\n",
    "        joined_rvw = \" \".join(initial_rvw)\n",
    "        \n",
    "        #Append review to the doc_list\n",
    "        doc_list.append(joined_rvw)\n",
    "        \n",
    "    return doc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T09:23:29.201391Z",
     "start_time": "2019-05-25T04:23:29.179802-05:00"
    }
   },
   "outputs": [],
   "source": [
    "def get_data_from_keras_imdb():\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    Extract TF-IDF matrices for the Keras IMDB dataset. \n",
    "    \n",
    "    \"\"\"\n",
    "    vocabulary_size = 1000\n",
    "    (X_train, y_train), (X_test, y_test) = imdb.load_data(num_words = vocabulary_size)\n",
    "    \n",
    "    #X = np.vstack([X_train[:, None], X_test[:, None]])\n",
    "    \n",
    "    X_train_docs = get_joined_rvw(X_train)\n",
    "    X_test_docs = get_joined_rvw(X_test)\n",
    "    \n",
    "    \n",
    "    tf_idf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2,\n",
    "                                   max_features=vocabulary_size,\n",
    "                                   stop_words='english')\n",
    "    \n",
    "    tf_idf_train = tf_idf_vectorizer.fit_transform(X_train_docs)\n",
    "    \n",
    "    tf_idf_test = tf_idf_vectorizer.fit_transform(X_test_docs)\n",
    "    \n",
    "    #tf_idf_feature_names = tf_idf_vectorizer.get_feature_names() \n",
    "    \n",
    "    #tf_idf = np.vstack([tf_idf_train.toarray(), tf_idf_test.toarray()])\n",
    "    \n",
    "    #X_new = pd.DataFrame(tf_idf, columns=tf_idf_feature_names)\n",
    "    \n",
    "    X_train_new = tf_idf_train.toarray()\n",
    "    \n",
    "    X_test_new = tf_idf_test.toarray()\n",
    "\n",
    "    \n",
    "    return X_train_new, y_train, X_test_new, y_test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T09:23:51.566936Z",
     "start_time": "2019-05-25T04:23:29.204508-05:00"
    }
   },
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test  = get_data_from_keras_imdb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T09:23:51.585902Z",
     "start_time": "2019-05-25T04:23:51.575550-05:00"
    }
   },
   "outputs": [],
   "source": [
    "print('train dataset shape', X_train.shape)\n",
    "print('test dataset shape', X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can readily see that we are ready to train our classification algorithm with the TF-IDF matrices. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T09:24:02.217253Z",
     "start_time": "2019-05-25T04:23:51.594177-05:00"
    }
   },
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_estimators=200, max_depth=3, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T09:24:02.247522Z",
     "start_time": "2019-05-25T04:24:02.220743-05:00"
    }
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred))\n",
    "print('Accuracy score : ', accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-05-25T09:23:15.965Z"
    }
   },
   "outputs": [],
   "source": [
    "model = MLPClassifier()\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "print('Accuracy score : ', accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-05-25T09:23:15.972Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "cross_val_score(model, X_train, y_train, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-05-25T09:23:15.977Z"
    }
   },
   "outputs": [],
   "source": [
    "import manu_utils as TCD\n",
    "palette = TCD.palette(cmap = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-05-25T09:23:15.993Z"
    }
   },
   "outputs": [],
   "source": [
    "C = confusion_matrix(y_test, y_pred)\n",
    "c_normed = C / C.astype(np.float).sum(axis=1) [:, np.newaxis]\n",
    "\n",
    "sns.heatmap(c_normed, cmap = palette, xticklabels=['negative', 'positive'], \n",
    "           yticklabels=['negative', 'positive'], annot= True, vmin = 0, vmax = 1, \n",
    "           cbar_kws = {'label': 'recall'})\n",
    "\n",
    "#\n",
    "\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
